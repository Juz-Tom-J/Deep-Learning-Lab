{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad64pzMD4Iwb",
        "outputId": "aa142671-7150-46a1-e317-d68b4c515d85"
      },
      "id": "ad64pzMD4Iwb",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "73c48214",
      "metadata": {
        "id": "73c48214"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from keras.layers import Input, LSTM, Embedding, Dense\n",
        "from keras.models import Model\n",
        "\n",
        "eng_hin = \"/content/drive/My Drive/Colab Notebooks/English_Hindi_Clean_New.csv\"\n",
        "data=pd.read_csv(eng_hin, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "d2170170",
      "metadata": {
        "id": "d2170170"
      },
      "outputs": [],
      "source": [
        "# Get English and Hindi Vocabulary\n",
        "all_eng_words = set()\n",
        "for eng in data['English']:\n",
        "    for word in eng.split():\n",
        "        if word not in all_eng_words:\n",
        "            all_eng_words.add(word)\n",
        "\n",
        "all_hin_words = set()\n",
        "for hin in data['Hindi']:\n",
        "    for word in hin.split():\n",
        "        if word not in all_hin_words:\n",
        "            all_hin_words.add(word)\n",
        "\n",
        "data['len_eng_sen'] = data['English'].apply(lambda x: len(x.split(\" \")))\n",
        "data['len_hin_sen'] = data['Hindi'].apply(lambda x: len(x.split(\" \")))\n",
        "\n",
        "data = data[data['len_eng_sen'] <= 20]\n",
        "data = data[data['len_hin_sen'] <= 20]\n",
        "\n",
        "max_len_src = max(data['len_hin_sen'])\n",
        "max_len_tar = max(data['len_eng_sen'])\n",
        "\n",
        "inp_words = sorted(list(all_eng_words))\n",
        "tar_words = sorted(list(all_hin_words))\n",
        "num_enc_toks = len(all_eng_words)\n",
        "num_dec_toks = len(all_hin_words) + 1  # for zero padding\n",
        "\n",
        "inp_tok_idx = dict((word, i + 1) for i, word in enumerate(inp_words))\n",
        "tar_tok_idx = dict((word, i + 1) for i, word in enumerate(tar_words))\n",
        "rev_inp_char_idx = dict((i, word) for word, i in inp_tok_idx.items())\n",
        "rev_tar_char_idx = dict((i, word) for word, i in tar_tok_idx.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ffcfce73",
      "metadata": {
        "id": "ffcfce73"
      },
      "outputs": [],
      "source": [
        "# Split the data into train and test\n",
        "X, y = data['English'], data['Hindi']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Increase batch size\n",
        "batch_size = 256\n",
        "\n",
        "# Generate batch data\n",
        "def generate_batch(X=X_train, y=y_train, batch_size=batch_size):\n",
        "    while True:\n",
        "        for j in range(0, len(X), batch_size):\n",
        "            enc_inp_data = np.zeros((batch_size, max_len_src), dtype='float32')\n",
        "            dec_inp_data = np.zeros((batch_size, max_len_tar), dtype='float32')\n",
        "            dec_tar_data = np.zeros((batch_size, max_len_tar, num_dec_toks), dtype='float32')\n",
        "            for i, (inp_text, tar_text) in enumerate(zip(X[j:j + batch_size], y[j:j + batch_size])):\n",
        "                for t, word in enumerate(inp_text.split()):\n",
        "                    enc_inp_data[i, t] = inp_tok_idx[word]\n",
        "                for t, word in enumerate(tar_text.split()):\n",
        "                    if t < len(tar_text.split()) - 1:\n",
        "                        dec_inp_data[i, t] = tar_tok_idx[word]\n",
        "                    if t > 0:\n",
        "                        dec_tar_data[i, t - 1, tar_tok_idx[word]] = 1.0\n",
        "            yield [enc_inp_data, dec_inp_data], dec_tar_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "d83fab11",
      "metadata": {
        "id": "d83fab11"
      },
      "outputs": [],
      "source": [
        "# Encoder-Decoder Architecture\n",
        "latent_dim = 250\n",
        "\n",
        "# Encoder\n",
        "enc_inps = Input(shape=(None,))\n",
        "enc_emb = Embedding(num_enc_toks, latent_dim, mask_zero=True)(enc_inps)\n",
        "enc_lstm = LSTM(latent_dim, return_state=True)\n",
        "enc_outputs, st_h, st_c = enc_lstm(enc_emb)\n",
        "enc_states = [st_h, st_c]\n",
        "\n",
        "# Set up the decoder\n",
        "dec_inps = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(num_dec_toks, latent_dim, mask_zero=True)\n",
        "dec_emb = dec_emb_layer(dec_inps)\n",
        "dec_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "dec_outputs, _, _ = dec_lstm(dec_emb, initial_state=enc_states)\n",
        "dec_dense = Dense(num_dec_toks, activation='softmax')\n",
        "dec_outputs = dec_dense(dec_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "97df78b1",
      "metadata": {
        "id": "97df78b1"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "model = Model([enc_inps, dec_inps], dec_outputs)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')  # Use Adam optimizer for faster convergence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "31bd1ae5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31bd1ae5",
        "outputId": "5bfbe26f-43d0-47ae-c05c-776b3a75a3d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "48/48 [==============================] - 25s 516ms/step - loss: 1.1823 - val_loss: 6.9228\n",
            "Epoch 2/50\n",
            "48/48 [==============================] - 24s 512ms/step - loss: 1.0950 - val_loss: 6.9634\n",
            "Epoch 3/50\n",
            "48/48 [==============================] - 24s 493ms/step - loss: 1.0518 - val_loss: 7.0207\n",
            "Epoch 4/50\n",
            "48/48 [==============================] - 25s 523ms/step - loss: 1.0209 - val_loss: 7.0337\n",
            "Epoch 5/50\n",
            "48/48 [==============================] - 23s 483ms/step - loss: 0.9922 - val_loss: 7.0718\n",
            "Epoch 6/50\n",
            "48/48 [==============================] - 25s 532ms/step - loss: 0.9684 - val_loss: 7.1193\n",
            "Epoch 7/50\n",
            "48/48 [==============================] - 22s 469ms/step - loss: 0.9396 - val_loss: 7.1585\n",
            "Epoch 8/50\n",
            "48/48 [==============================] - 26s 543ms/step - loss: 0.9073 - val_loss: 7.1748\n",
            "Epoch 9/50\n",
            "48/48 [==============================] - 26s 543ms/step - loss: 0.8783 - val_loss: 7.1784\n",
            "Epoch 10/50\n",
            "48/48 [==============================] - 24s 502ms/step - loss: 0.8526 - val_loss: 7.2157\n",
            "Epoch 11/50\n",
            "48/48 [==============================] - 26s 541ms/step - loss: 0.8192 - val_loss: 7.2804\n",
            "Epoch 12/50\n",
            "48/48 [==============================] - 25s 518ms/step - loss: 0.7963 - val_loss: 7.3223\n",
            "Epoch 13/50\n",
            "48/48 [==============================] - 26s 538ms/step - loss: 0.7769 - val_loss: 7.3243\n",
            "Epoch 14/50\n",
            "48/48 [==============================] - 25s 519ms/step - loss: 0.7478 - val_loss: 7.3457\n",
            "Epoch 15/50\n",
            "48/48 [==============================] - 23s 476ms/step - loss: 0.7161 - val_loss: 7.4050\n",
            "Epoch 16/50\n",
            "48/48 [==============================] - 23s 490ms/step - loss: 0.6857 - val_loss: 7.4265\n",
            "Epoch 17/50\n",
            "48/48 [==============================] - 22s 454ms/step - loss: 0.6611 - val_loss: 7.4774\n",
            "Epoch 18/50\n",
            "48/48 [==============================] - 24s 514ms/step - loss: 0.6357 - val_loss: 7.5121\n",
            "Epoch 19/50\n",
            "48/48 [==============================] - 22s 466ms/step - loss: 0.6099 - val_loss: 7.4890\n",
            "Epoch 20/50\n",
            "48/48 [==============================] - 23s 482ms/step - loss: 0.5852 - val_loss: 7.5292\n",
            "Epoch 21/50\n",
            "48/48 [==============================] - 26s 538ms/step - loss: 0.5631 - val_loss: 7.5591\n",
            "Epoch 22/50\n",
            "48/48 [==============================] - 26s 552ms/step - loss: 0.5410 - val_loss: 7.6163\n",
            "Epoch 23/50\n",
            "48/48 [==============================] - 27s 562ms/step - loss: 0.5232 - val_loss: 7.7137\n",
            "Epoch 24/50\n",
            "48/48 [==============================] - 25s 517ms/step - loss: 0.5073 - val_loss: 7.7185\n",
            "Epoch 25/50\n",
            "48/48 [==============================] - 22s 469ms/step - loss: 0.4918 - val_loss: 7.7299\n",
            "Epoch 26/50\n",
            "48/48 [==============================] - 26s 536ms/step - loss: 0.4786 - val_loss: 7.7260\n",
            "Epoch 27/50\n",
            "48/48 [==============================] - 22s 469ms/step - loss: 0.4679 - val_loss: 7.6913\n",
            "Epoch 28/50\n",
            "48/48 [==============================] - 23s 486ms/step - loss: 0.4557 - val_loss: 7.7574\n",
            "Epoch 29/50\n",
            "48/48 [==============================] - 23s 484ms/step - loss: 0.4392 - val_loss: 7.7910\n",
            "Epoch 30/50\n",
            "48/48 [==============================] - 26s 542ms/step - loss: 0.4272 - val_loss: 7.8656\n",
            "Epoch 31/50\n",
            "48/48 [==============================] - 22s 465ms/step - loss: 0.4166 - val_loss: 7.8734\n",
            "Epoch 32/50\n",
            "48/48 [==============================] - 22s 470ms/step - loss: 0.3967 - val_loss: 7.8436\n",
            "Epoch 33/50\n",
            "48/48 [==============================] - 24s 499ms/step - loss: 0.3736 - val_loss: 7.8690\n",
            "Epoch 34/50\n",
            "48/48 [==============================] - 22s 466ms/step - loss: 0.3505 - val_loss: 7.8768\n",
            "Epoch 35/50\n",
            "48/48 [==============================] - 25s 523ms/step - loss: 0.3339 - val_loss: 7.9421\n",
            "Epoch 36/50\n",
            "48/48 [==============================] - 26s 546ms/step - loss: 0.3157 - val_loss: 7.9894\n",
            "Epoch 37/50\n",
            "48/48 [==============================] - 26s 557ms/step - loss: 0.3020 - val_loss: 8.0098\n",
            "Epoch 38/50\n",
            "48/48 [==============================] - 25s 522ms/step - loss: 0.2844 - val_loss: 8.0389\n",
            "Epoch 39/50\n",
            "48/48 [==============================] - 26s 547ms/step - loss: 0.2716 - val_loss: 8.0658\n",
            "Epoch 40/50\n",
            "48/48 [==============================] - 26s 538ms/step - loss: 0.2615 - val_loss: 8.0966\n",
            "Epoch 41/50\n",
            "48/48 [==============================] - 22s 471ms/step - loss: 0.2513 - val_loss: 8.1129\n",
            "Epoch 42/50\n",
            "48/48 [==============================] - 24s 499ms/step - loss: 0.2395 - val_loss: 8.1478\n",
            "Epoch 43/50\n",
            "48/48 [==============================] - 26s 546ms/step - loss: 0.2314 - val_loss: 8.1710\n",
            "Epoch 44/50\n",
            "48/48 [==============================] - 26s 543ms/step - loss: 0.2241 - val_loss: 8.2199\n",
            "Epoch 45/50\n",
            "48/48 [==============================] - 25s 530ms/step - loss: 0.2159 - val_loss: 8.2659\n",
            "Epoch 46/50\n",
            "48/48 [==============================] - 24s 497ms/step - loss: 0.2089 - val_loss: 8.2784\n",
            "Epoch 47/50\n",
            "48/48 [==============================] - 22s 459ms/step - loss: 0.2009 - val_loss: 8.3102\n",
            "Epoch 48/50\n",
            "48/48 [==============================] - 24s 500ms/step - loss: 0.1960 - val_loss: 8.3048\n",
            "Epoch 49/50\n",
            "48/48 [==============================] - 26s 548ms/step - loss: 0.1902 - val_loss: 8.3275\n",
            "Epoch 50/50\n",
            "48/48 [==============================] - 27s 561ms/step - loss: 0.1877 - val_loss: 8.3843\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ef62f2563b0>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "train_samples = len(X_train)\n",
        "val_samples = len(X_test)\n",
        "\n",
        "# Train the model with a larger batch size\n",
        "model.fit(x=generate_batch(X_train, y_train, batch_size=batch_size),\n",
        "          steps_per_epoch=train_samples // batch_size,\n",
        "          epochs=50,\n",
        "          validation_data=generate_batch(X_test, y_test, batch_size=batch_size),\n",
        "          validation_steps=val_samples // batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "45e0d7cf",
      "metadata": {
        "id": "45e0d7cf"
      },
      "outputs": [],
      "source": [
        "# Encode the input sequence to get the \"thought vectors\"\n",
        "enc_model = Model(enc_inps, enc_states)\n",
        "\n",
        "# Decoder setup\n",
        "# Below tensors will hold the states of the previous time step\n",
        "dec_st_inp_h = Input(shape=(latent_dim,))\n",
        "dec_st_inp_c = Input(shape=(latent_dim,))\n",
        "dec_states_inps = [dec_st_inp_h, dec_st_inp_c]\n",
        "\n",
        "dec_emb2= dec_emb_layer(dec_inps) # Get the embeddings of the decoder sequence\n",
        "\n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "dec_outputs2, st_h2, st_c2 = dec_lstm(dec_emb2, initial_state=dec_states_inps)\n",
        "dec_states2 = [st_h2, st_c2]\n",
        "dec_outputs2 = dec_dense(dec_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "\n",
        "# Final decoder model\n",
        "dec_model = Model(\n",
        "    [dec_inps] + dec_states_inps,\n",
        "    [dec_outputs2] + dec_states2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "ec0c48cb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec0c48cb",
        "outputId": "1275709a-1aa9-4890-a076-17b0784c962c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Input English sentence: which is a pity but in india every other sport\n",
            "\n",
            "          Predicted Hindi Translation:  जिसपे हमें तरस आती है लेकिन भारत में हर खेल \n",
            "\n",
            "          Actual Hindi Translation:  जिसपे हमें तरस आती है लेकिन भारत में हर खेल \n"
          ]
        }
      ],
      "source": [
        "def translate(inp_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = enc_model.predict(inp_seq)\n",
        "    # Generate empty target sequence of length 1.\n",
        "    tar_seq = np.zeros((1,1))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    tar_seq[0, 0] = tar_tok_idx['START_']\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_cond = False\n",
        "    dec_sen = ''\n",
        "    while not stop_cond:\n",
        "        output_toks, h, c = dec_model.predict([tar_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_tok_idx = np.argmax(output_toks[0, -1, :])\n",
        "        sampled_char = rev_tar_char_idx[sampled_tok_idx]\n",
        "        dec_sen += ' '+sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_char == '_END' or\n",
        "           len(dec_sen) > 50):\n",
        "            stop_cond = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        tar_seq = np.zeros((1,1))\n",
        "        tar_seq[0, 0] = sampled_tok_idx\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return dec_sen\n",
        "\n",
        "train_gen = generate_batch(X_train, y_train, batch_size = 1)\n",
        "k=0\n",
        "(inp_seq, actual_output), _ = next(train_gen)\n",
        "hin_sen = translate(inp_seq)\n",
        "print(f'''Input English sentence: {X_train[k:k+1].values[0]}\\n\n",
        "          Predicted Hindi Translation: {hin_sen[:-4]}\\n\n",
        "          Actual Hindi Translation: {y_train[k:k+1].values[0][6:-4]}''')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}