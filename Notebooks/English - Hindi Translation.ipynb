{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f43d3760",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "eng_hin = 'English_Hindi_Clean.csv'\n",
    "data=pd.read_csv(eng_hin, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64a4c2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get English and Hindi Vocabulary\n",
    "all_eng_words = set()\n",
    "for eng in data['English']:\n",
    "    for word in eng.split():\n",
    "        if word not in all_eng_words:\n",
    "            all_eng_words.add(word)\n",
    "\n",
    "all_hin_words = set()\n",
    "for hin in data['Hindi']:\n",
    "    for word in hin.split():\n",
    "        if word not in all_hin_words:\n",
    "            all_hin_words.add(word)\n",
    "\n",
    "data['len_eng_sen'] = data['English'].apply(lambda x: len(x.split(\" \")))\n",
    "data['len_hin_sen'] = data['Hindi'].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "data = data[data['len_eng_sen'] <= 20]\n",
    "data = data[data['len_hin_sen'] <= 20]\n",
    "\n",
    "max_len_src = max(data['len_hin_sen'])\n",
    "max_len_tar = max(data['len_eng_sen'])\n",
    "\n",
    "inp_words = sorted(list(all_eng_words))\n",
    "tar_words = sorted(list(all_hin_words))\n",
    "num_enc_toks = len(all_eng_words)\n",
    "num_dec_toks = len(all_hin_words) + 1  # for zero padding\n",
    "\n",
    "inp_tok_idx = dict((word, i + 1) for i, word in enumerate(inp_words))\n",
    "tar_tok_idx = dict((word, i + 1) for i, word in enumerate(tar_words))\n",
    "rev_inp_char_idx = dict((i, word) for word, i in inp_tok_idx.items())\n",
    "rev_tar_char_idx = dict((i, word) for word, i in tar_tok_idx.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c814c0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test\n",
    "X, y = data['English'], data['Hindi']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Increase batch size\n",
    "batch_size = 256\n",
    "\n",
    "# Generate batch data\n",
    "def generate_batch(X=X_train, y=y_train, batch_size=batch_size):\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            enc_inp_data = np.zeros((batch_size, max_len_src), dtype='float32')\n",
    "            dec_inp_data = np.zeros((batch_size, max_len_tar), dtype='float32')\n",
    "            dec_tar_data = np.zeros((batch_size, max_len_tar, num_dec_toks), dtype='float32')\n",
    "            for i, (inp_text, tar_text) in enumerate(zip(X[j:j + batch_size], y[j:j + batch_size])):\n",
    "                for t, word in enumerate(inp_text.split()):\n",
    "                    enc_inp_data[i, t] = inp_tok_idx[word]\n",
    "                for t, word in enumerate(tar_text.split()):\n",
    "                    if t < len(tar_text.split()) - 1:\n",
    "                        dec_inp_data[i, t] = tar_tok_idx[word]\n",
    "                    if t > 0:\n",
    "                        dec_tar_data[i, t - 1, tar_tok_idx[word]] = 1.0\n",
    "            yield [enc_inp_data, dec_inp_data], dec_tar_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3663e631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder-Decoder Architecture\n",
    "latent_dim = 250\n",
    "\n",
    "# Encoder\n",
    "enc_inps = Input(shape=(None,))\n",
    "enc_emb = Embedding(num_enc_toks, latent_dim, mask_zero=True)(enc_inps)\n",
    "enc_lstm = LSTM(latent_dim, return_state=True)\n",
    "enc_outputs, st_h, st_c = enc_lstm(enc_emb)\n",
    "enc_states = [st_h, st_c]\n",
    "\n",
    "# Set up the decoder\n",
    "dec_inps = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(num_dec_toks, latent_dim, mask_zero=True)\n",
    "dec_emb = dec_emb_layer(dec_inps)\n",
    "dec_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "dec_outputs, _, _ = dec_lstm(dec_emb, initial_state=enc_states)\n",
    "dec_dense = Dense(num_dec_toks, activation='softmax')\n",
    "dec_outputs = dec_dense(dec_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ac7414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Model([enc_inps, dec_inps], dec_outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')  # Use Adam optimizer for faster convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69ef06ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "104/123 [========================>.....] - ETA: 28s - loss: 7.1489"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node model/embedding/embedding_lookup defined at (most recent call last):\n  File \"C:\\ProgramData\\anaconda3\\lib\\runpy.py\", line 196, in _run_module_as_main\n\n  File \"C:\\ProgramData\\anaconda3\\lib\\runpy.py\", line 86, in _run_code\n\n  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n\n  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 711, in start\n\n  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n\n  File \"C:\\ProgramData\\anaconda3\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n\n  File \"C:\\ProgramData\\anaconda3\\lib\\asyncio\\base_events.py\", line 1906, in _run_once\n\n  File \"C:\\ProgramData\\anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n\n  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n\n  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n\n  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 729, in execute_request\n\n  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 411, in do_execute\n\n  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 531, in run_cell\n\n  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2961, in run_cell\n\n  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3016, in _run_cell\n\n  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3221, in run_cell_async\n\n  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3400, in run_ast_nodes\n\n  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3460, in run_code\n\n  File \"C:\\Users\\Student\\AppData\\Local\\Temp\\ipykernel_11748\\4131933289.py\", line 5, in <module>\n\n  File \"C:\\Users\\Student\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"C:\\Users\\Student\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\training.py\", line 1783, in fit\n\n  File \"C:\\Users\\Student\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\training.py\", line 1377, in train_function\n\n  File \"C:\\Users\\Student\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\training.py\", line 1360, in step_function\n\n  File \"C:\\Users\\Student\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\training.py\", line 1349, in run_step\n\n  File \"C:\\Users\\Student\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\training.py\", line 1126, in train_step\n\n  File \"C:\\Users\\Student\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"C:\\Users\\Student\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\training.py\", line 589, in __call__\n\n  File \"C:\\Users\\Student\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"C:\\Users\\Student\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1149, in __call__\n\n  File \"C:\\Users\\Student\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n\n  File \"C:\\Users\\Student\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\functional.py\", line 515, in call\n\n  File \"C:\\Users\\Student\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\functional.py\", line 672, in _run_internal_graph\n\n  File \"C:\\Users\\Student\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"C:\\Users\\Student\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1149, in __call__\n\n  File \"C:\\Users\\Student\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n\n  File \"C:\\Users\\Student\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\layers\\core\\embedding.py\", line 272, in call\n\nindices[78,0] = 17234 is not in [0, 17234)\n\t [[{{node model/embedding/embedding_lookup}}]] [Op:__inference_train_function_14441]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m val_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(X_test)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Train the model with a larger batch size\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerate_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m          \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerate_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[0;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[0;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[0;32m     59\u001b[0m   ]\n\u001b[1;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     61\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node model/embedding/embedding_lookup defined at (most recent call last):\n  File \"C:\\ProgramData\\anaconda3\\lib\\runpy.py\", line 196, in _run_module_as_main\n\n  File \"C:\\ProgramData\\anaconda3\\lib\\runpy.py\", line 86, in _run_code\n\n  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n\n  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 711, in start\n\n  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n\n  File \"C:\\ProgramData\\anaconda3\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n\n  File \"C:\\ProgramData\\anaconda3\\lib\\asyncio\\base_events.py\", line 1906, in _run_once\n\n  File \"C:\\ProgramData\\anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n\n  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n\n  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n\n  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 729, in execute_request\n\n  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 411, in do_execute\n\n  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 531, in run_cell\n\n  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2961, in run_cell\n\n  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3016, in _run_cell\n\n  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3221, in run_cell_async\n\n  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3400, in run_ast_nodes\n\n  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3460, in run_code\n\n  File \"C:\\Users\\Student\\AppData\\Local\\Temp\\ipykernel_11748\\4131933289.py\", line 5, in <module>\n\n  File \"C:\\Users\\Student\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"C:\\Users\\Student\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\training.py\", line 1783, in fit\n\n  File \"C:\\Users\\Student\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\training.py\", line 1377, in train_function\n\n  File \"C:\\Users\\Student\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\training.py\", line 1360, in step_function\n\n  File \"C:\\Users\\Student\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\training.py\", line 1349, in run_step\n\n  File \"C:\\Users\\Student\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\training.py\", line 1126, in train_step\n\n  File \"C:\\Users\\Student\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"C:\\Users\\Student\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\training.py\", line 589, in __call__\n\n  File \"C:\\Users\\Student\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"C:\\Users\\Student\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1149, in __call__\n\n  File \"C:\\Users\\Student\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n\n  File \"C:\\Users\\Student\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\functional.py\", line 515, in call\n\n  File \"C:\\Users\\Student\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\functional.py\", line 672, in _run_internal_graph\n\n  File \"C:\\Users\\Student\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"C:\\Users\\Student\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1149, in __call__\n\n  File \"C:\\Users\\Student\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n\n  File \"C:\\Users\\Student\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\layers\\core\\embedding.py\", line 272, in call\n\nindices[78,0] = 17234 is not in [0, 17234)\n\t [[{{node model/embedding/embedding_lookup}}]] [Op:__inference_train_function_14441]"
     ]
    }
   ],
   "source": [
    "train_samples = len(X_train)\n",
    "val_samples = len(X_test)\n",
    "\n",
    "# Train the model with a larger batch size\n",
    "model.fit(x=generate_batch(X_train, y_train, batch_size=batch_size),\n",
    "          steps_per_epoch=train_samples // batch_size,\n",
    "          epochs=50,\n",
    "          validation_data=generate_batch(X_test, y_test, batch_size=batch_size),\n",
    "          validation_steps=val_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69d7df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the \"thought vectors\"\n",
    "enc_model = Model(enc_inps, enc_states)\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "dec_st_inp_h = Input(shape=(latent_dim,))\n",
    "dec_st_inp_c = Input(shape=(latent_dim,))\n",
    "dec_states_inps = [dec_st_inp_h, dec_st_inp_c]\n",
    "\n",
    "dec_emb2= dec_emb_layer(dec_inps) # Get the embeddings of the decoder sequence\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "dec_outputs2, st_h2, st_c2 = dec_lstm(dec_emb2, initial_state=dec_states_inps)\n",
    "dec_states2 = [st_h2, st_c2]\n",
    "dec_outputs2 = dec_dense(dec_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "\n",
    "# Final decoder model\n",
    "dec_model = Model(\n",
    "    [dec_inps] + dec_states_inps,\n",
    "    [dec_outputs2] + dec_states2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba55065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(inp_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = enc_model.predict(inp_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    tar_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    tar_seq[0, 0] = tar_tok_idx['START_']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_cond = False\n",
    "    dec_sen = ''\n",
    "    while not stop_cond:\n",
    "        output_toks, h, c = dec_model.predict([tar_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_tok_idx = np.argmax(output_toks[0, -1, :])\n",
    "        sampled_char = rev_tar_char_idx[sampled_tok_idx]\n",
    "        dec_sen += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '_END' or\n",
    "           len(dec_sen) > 50):\n",
    "            stop_cond = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        tar_seq = np.zeros((1,1))\n",
    "        tar_seq[0, 0] = sampled_tok_idx\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return dec_sen\n",
    "\n",
    "train_gen = generate_batch(X_train, y_train, batch_size = 1)\n",
    "k=0\n",
    "(inp_seq, actual_output), _ = next(train_gen)\n",
    "hin_sen = translate(inp_seq)\n",
    "print(f'''Input English sentence: {X_train[k:k+1].values[0]}\\n\n",
    "          Predicted Hindi Translation: {hin_sen[:-4]}\\n\n",
    "          Actual Hindi Translation: {y_train[k:k+1].values[0][6:-4]}''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
